15/12/15 16:55:20 DEBUG yarn.ApplicationMaster: Sending progress
15/12/15 16:55:20 TRACE ipc.ProtobufRpcEngine: 27: Call -> host-40/172.22.151.235:8030: allocate {blacklist_request { } response_id: 16 progress: 0.1}
15/12/15 16:55:20 DEBUG ipc.Client: IPC Client (1192487936) connection to host-40/172.22.151.235:8030 from hct sending #30
15/12/15 16:55:20 DEBUG ipc.Client: IPC Client (1192487936) connection to host-40/172.22.151.235:8030 from hct got value #30
15/12/15 16:55:20 DEBUG ipc.ProtobufRpcEngine: Call: allocate took 2ms
15/12/15 16:55:20 TRACE ipc.ProtobufRpcEngine: 27: Response <- host-40/172.22.151.235:8030: allocate {response_id: 17 completed_container_statuses { container_id { app_attempt_id { application_id { id: 205 cluster_timestamp: 1449751495184 } attemptId: 1 } id: 5 } state: C_COMPLETE diagnostics: "Container killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\nKilled by external signal\n" exit_status: 143 } limit { memory: 7168 virtual_cores: 1 } num_cluster_nodes: 20}
15/12/15 16:55:20 DEBUG yarn.YarnAllocator: Completed 1 containers
15/12/15 16:55:20 INFO yarn.YarnAllocator: Completed container container_1449751495184_0205_01_000005 on host: host-55 (state: COMPLETE, exit status: 143)
15/12/15 16:55:20 WARN yarn.YarnAllocator: Container marked as failed: container_1449751495184_0205_01_000005 on host: host-55. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal

15/12/15 16:55:20 TRACE server.TransportRequestHandler: Sent result RpcResponse{requestId=5442625892926054939, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=445 cap=720]}} to client host-40/172.22.151.235:55871
15/12/15 16:55:20 DEBUG yarn.YarnAllocator: Finished processing 1 completed containers. Current running executor count: 12.
15/12/15 16:55:20 DEBUG yarn.ApplicationMaster: Number of pending allocations is 0. Sleeping for 3000.
15/12/15 16:55:23 DEBUG yarn.ApplicationMaster: Sending progress
15/12/15 16:55:23 INFO yarn.YarnAllocator: Not the initial request. 1 executor containers will be requested in 10 seconds
15/12/15 16:55:23 TRACE ipc.ProtobufRpcEngine: 27: Call -> host-40/172.22.151.235:8030: allocate {blacklist_request { } response_id: 17 progress: 0.1}
15/12/15 16:55:23 DEBUG ipc.Client: IPC Client (1192487936) connection to host-40/172.22.151.235:8030 from hct sending #31
15/12/15 16:55:23 DEBUG ipc.Client: IPC Client (1192487936) connection to host-40/172.22.151.235:8030 from hct got value #31
15/12/15 16:55:23 DEBUG ipc.ProtobufRpcEngine: Call: allocate took 2ms
15/12/15 16:55:23 TRACE ipc.ProtobufRpcEngine: 27: Response <- host-40/172.22.151.235:8030: allocate {response_id: 18 limit { memory: 7168 virtual_cores: 1 } num_cluster_nodes: 20}
15/12/15 16:55:23 DEBUG yarn.ApplicationMaster: Number of pending allocations is 0. Sleeping for 3000.
15/12/15 16:55:26 DEBUG yarn.ApplicationMaster: Sending progress
15/12/15 16:55:26 INFO yarn.YarnAllocator: Will request 1 executor containers, each with 8 cores and 6758 MB memory including 614 MB overhead
15/12/15 16:55:26 DEBUG impl.AMRMClientImpl: Added priority=1
15/12/15 16:55:26 DEBUG impl.AMRMClientImpl: addResourceRequest: applicationId= priority=1 resourceName=* numContainers=1 #asks=1
15/12/15 16:55:26 INFO yarn.YarnAllocator: Container request (host: Any, capability: <memory:6758, vCores:8>)
15/12/15 16:55:26 TRACE ipc.ProtobufRpcEngine: 27: Call -> host-40/172.22.151.235:8030: allocate {ask { priority { priority: 1 } resource_name: "*" capability { memory: 6758 virtual_cores: 8 } num_containers: 1 relax_locality: true } blacklist_request { } response_id: 18 progress: 0.1}
15/12/15 16:55:26 DEBUG ipc.Client: IPC Client (1192487936) connection to host-40/172.22.151.235:8030 from hct sending #32
15/12/15 16:55:26 DEBUG ipc.Client: IPC Client (1192487936) connection to host-40/172.22.151.235:8030 from hct got value #32
15/12/15 16:55:26 DEBUG ipc.ProtobufRpcEngine: Call: allocate took 4ms
15/12/15 16:55:26 TRACE ipc.ProtobufRpcEngine: 27: Response <- host-40/172.22.151.235:8030: allocate {response_id: 19 limit { memory: 7168 virtual_cores: 1 } num_cluster_nodes: 20}
15/12/15 16:55:26 DEBUG yarn.ApplicationMaster: Number of pending allocations is 1. Sleeping for 200.
15/12/15 16:55:26 DEBUG yarn.ApplicationMaster: Sending progress
15/12/15 16:55:26 TRACE ipc.ProtobufRpcEngine: 27: Call -> host-40/172.22.151.235:8030: allocate {blacklist_request { } response_id: 19 progress: 0.1}
15/12/15 16:55:26 DEBUG ipc.Client: IPC Client (1192487936) connection to host-40/172.22.151.235:8030 from hct sending #33
15/12/15 16:55:26 DEBUG ipc.Client: IPC Client (1192487936) connection to host-40/172.22.151.235:8030 from hct got value #33
15/12/15 16:55:26 DEBUG ipc.ProtobufRpcEngine: Call: allocate took 2ms
15/12/15 16:55:26 TRACE ipc.ProtobufRpcEngine: 27: Response <- host-40/172.22.151.235:8030: allocate {response_id: 20 limit { memory: 7168 virtual_cores: 1 } num_cluster_nodes: 20}
15/12/15 16:55:26 DEBUG yarn.ApplicationMaster: Number of pending allocations is 1. Sleeping for 400.
15/12/15 16:55:27 DEBUG yarn.ApplicationMaster: Sending progress
15/12/15 16:55:27 TRACE ipc.ProtobufRpcEngine: 27: Call -> host-40/172.22.151.235:8030: allocate {blacklist_request { } response_id: 20 progress: 0.1}
15/12/15 16:55:27 DEBUG ipc.Client: IPC Client (1192487936) connection to host-40/172.22.151.235:8030 from hct sending #34
15/12/15 16:55:27 DEBUG ipc.Client: IPC Client (1192487936) connection to host-40/172.22.151.235:8030 from hct got value #34
15/12/15 16:55:27 DEBUG ipc.ProtobufRpcEngine: Call: allocate took 3ms
15/12/15 16:55:27 TRACE ipc.ProtobufRpcEngine: 27: Response <- host-40/172.22.151.235:8030: allocate {response_id: 21 allocated_containers { id { app_attempt_id { application_id { id: 205 cluster_timestamp: 1449751495184 } attemptId: 1 } id: 62 } nodeId { host: "host-55" port: 50582 } node_http_address: "host-55:8042" resource { memory: 7168 virtual_cores: 1 } priority { priority: 1 } container_token { identifier: "\n\022\022\016\n\n\b\315\001\020\220\204\261\337\230*\020\001\030>\022!host-55:50582\032\003hct\"\005\b\2008\020\001(\332\302\214\247\232*0\332\360\365\367\0048\220\204\261\337\230*B\002\b\001H\322\361\347\246\232*" password: "L\370\v0\304\267\260\v\277x\347=;G\02661\034\203" kind: "ContainerToken" service: "172.22.151.250:50582" } } limit { memory: 7168 virtual_cores: 1 } num_cluster_nodes: 20}
15/12/15 16:55:27 DEBUG yarn.YarnAllocator: Allocated containers: 1. Current executor count: 12. Cluster resources: <memory:7168, vCores:1>.
15/12/15 16:55:27 DEBUG impl.AMRMClientImpl: BEFORE decResourceRequest: applicationId= priority=1 resourceName=* numContainers=1 #asks=0
15/12/15 16:55:27 INFO impl.AMRMClientImpl: AFTER decResourceRequest: applicationId= priority=1 resourceName=* numContainers=0 #asks=1
15/12/15 16:55:27 INFO yarn.YarnAllocator: Launching container container_1449751495184_0205_01_000062 for on host host-55
15/12/15 16:55:27 DEBUG ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
15/12/15 16:55:27 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@172.22.151.235:55871,  executorHostname: host-55
15/12/15 16:55:27 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
15/12/15 16:55:27 DEBUG yarn.ApplicationMaster: Number of pending allocations is 0. Sleeping for 3000.
15/12/15 16:55:27 INFO yarn.ExecutorRunnable: Starting Executor Container
15/12/15 16:55:27 DEBUG service.AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.NMClientImpl entered state INITED
15/12/15 16:55:27 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
15/12/15 16:55:27 DEBUG ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
15/12/15 16:55:27 DEBUG service.AbstractService: Service org.apache.hadoop.yarn.client.api.impl.NMClientImpl is started
15/12/15 16:55:27 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
15/12/15 16:55:27 INFO yarn.ExecutorRunnable: Preparing Local resources
15/12/15 16:55:27 INFO yarn.ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "host-40" port: 9000 file: "/user/hct/.sparkStaging/application_1449751495184_0205/spark-assembly-1.6.0-SNAPSHOT-hadoop2.6.0.jar" } size: 145944869 timestamp: 1450169686113 type: FILE visibility: PRIVATE)
15/12/15 16:55:27 DEBUG yarn.Client: Using the default MR application classpath: $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*
15/12/15 16:55:27 INFO yarn.ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$TEZ_HOME/*<CPS>$TEZ_HOME/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*
    SPARK_LOG_URL_STDERR -> http://host-55:8042/node/containerlogs/container_1449751495184_0205_01_000062/hct/stderr?start=-4096
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1449751495184_0205
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 145944869
    SPARK_USER -> hct
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1450169686113
    SPARK_LOG_URL_STDOUT -> http://host-55:8042/node/containerlogs/container_1449751495184_0205_01_000062/hct/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://host-40:9000/user/hct/.sparkStaging/application_1449751495184_0205/spark-assembly-1.6.0-SNAPSHOT-hadoop2.6.0.jar#__spark__.jar

  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms6144m -Xmx6144m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=55871' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@172.22.151.235:55871 --executor-id 14 --hostname host-55 --cores 8 --app-id application_1449751495184_0205 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
      
15/12/15 16:55:27 INFO impl.ContainerManagementProtocolProxy: Opening proxy : host-55:50582
15/12/15 16:55:27 DEBUG security.SecurityUtil: Acquired token Kind: NMToken, Service: 172.22.151.250:50582, Ident: (appAttemptId { application_id { id: 205 cluster_timestamp: 1449751495184 } attemptId: 1 } nodeId { host: "host-55" port: 50582 } appSubmitter: "hct" keyId: 1657307667)
15/12/15 16:55:27 DEBUG security.UserGroupInformation: PrivilegedAction as:appattempt_1449751495184_0205_000001 (auth:SIMPLE) from:org.apache.hadoop.yarn.client.ServerProxy.createRetriableProxy(ServerProxy.java:88)
15/12/15 16:55:27 DEBUG ipc.HadoopYarnProtoRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ContainerManagementProtocol
15/12/15 16:55:27 DEBUG ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@6e4f7806
15/12/15 16:55:27 TRACE ipc.ProtobufRpcEngine: 55: Call -> host-55/172.22.151.250:50582: startContainers {start_container_request { container_launch_context { localResources { key: "__spark__.jar" value { resource { scheme: "hdfs" host: "host-40" port: 9000 file: "/user/hct/.sparkStaging/application_1449751495184_0205/spark-assembly-1.6.0-SNAPSHOT-hadoop2.6.0.jar" } size: 145944869 timestamp: 1450169686113 type: FILE visibility: PRIVATE } } tokens: "HDTS\000\001\000\033\n\016\n\n\b\315\001\020\220\204\261\337\230*\020\001\020\235\214\362\216\374\377\377\377\377\001\024)\341\241V\253\337\343uY\323/Q\251\372\224\236\312\226\216\330\020YARN_AM_RM_TOKEN\023172.22.151.235:8030\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000" environment { key: "SPARK_YARN_MODE" value: "true" } environment { key: "SPARK_YARN_CACHE_FILES_FILE_SIZES" value: "145944869" } environment { key: "SPARK_YARN_STAGING_DIR" value: ".sparkStaging/application_1449751495184_0205" } environment { key: "SPARK_YARN_CACHE_FILES" value: "hdfs://host-40:9000/user/hct/.sparkStaging/application_1449751495184_0205/spark-assembly-1.6.0-SNAPSHOT-hadoop2.6.0.jar#__spark__.jar" } environment { key: "SPARK_YARN_CACHE_FILES_VISIBILITIES" value: "PRIVATE" } environment { key: "SPARK_LOG_URL_STDOUT" value: "http://host-55:8042/node/containerlogs/container_1449751495184_0205_01_000062/hct/stdout?start=-4096" } environment { key: "SPARK_LOG_URL_STDERR" value: "http://host-55:8042/node/containerlogs/container_1449751495184_0205_01_000062/hct/stderr?start=-4096" } environment { key: "CLASSPATH" value: "{{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$TEZ_HOME/*<CPS>$TEZ_HOME/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*" } environment { key: "SPARK_YARN_CACHE_FILES_TIME_STAMPS" value: "1450169686113" } environment { key: "SPARK_USER" value: "hct" } command: "{{JAVA_HOME}}/bin/java" command: "-server" command: "-XX:OnOutOfMemoryError=\'kill %p\'" command: "-Xms6144m" command: "-Xmx6144m" command: "-Djava.io.tmpdir={{PWD}}/tmp" command: "\'-Dspark.driver.port=55871\'" command: "-Dspark.yarn.app.container.log.dir=<LOG_DIR>" command: "-XX:MaxPermSize=256m" command: "org.apache.spark.executor.CoarseGrainedExecutorBackend" command: "--driver-url" command: "spark://CoarseGrainedScheduler@172.22.151.235:55871" command: "--executor-id" command: "14" command: "--hostname" command: "host-55" command: "--cores" command: "8" command: "--app-id" command: "application_1449751495184_0205" command: "--user-class-path" command: "file:$PWD/__app__.jar" command: "1>" command: "<LOG_DIR>/stdout" command: "2>" command: "<LOG_DIR>/stderr" application_ACLs { accessType: APPACCESS_MODIFY_APP acl: "hct" } application_ACLs { accessType: APPACCESS_VIEW_APP acl: "hct" } } container_token { identifier: "\n\022\022\016\n\n\b\315\001\020\220\204\261\337\230*\020\001\030>\022!host-55:50582\032\003hct\"\005\b\2008\020\001(\332\302\214\247\232*0\332\360\365\367\0048\220\204\261\337\230*B\002\b\001H\322\361\347\246\232*" password: "L\370\v0\304\267\260\v\277x\347=;G\02661\034\203" kind: "ContainerToken" service: "172.22.151.250:50582" } }}
15/12/15 16:55:27 DEBUG ipc.Client: The ping interval is 60000 ms.
15/12/15 16:55:27 DEBUG ipc.Client: Connecting to host-55/172.22.151.250:50582
15/12/15 16:55:27 DEBUG security.UserGroupInformation: PrivilegedAction as:appattempt_1449751495184_0205_000001 (auth:SIMPLE) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:717)
15/12/15 16:55:27 DEBUG security.SaslRpcClient: Sending sasl message state: NEGOTIATE

15/12/15 16:55:27 DEBUG security.SaslRpcClient: Received SASL message state: NEGOTIATE
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
  challenge: "realm=\"default\",nonce=\"SdoqyMQak5MKLXFZUGBc0NNr/+mIwiu9KkMJnO7Z\",qop=\"auth\",charset=utf-8,algorithm=md5-sess"
}

15/12/15 16:55:27 DEBUG security.SaslRpcClient: Get token info proto:interface org.apache.hadoop.yarn.api.ContainerManagementProtocolPB info:org.apache.hadoop.yarn.security.ContainerManagerSecurityInfo$1@32601ddc
15/12/15 16:55:27 INFO security.NMTokenSelector: Looking for service: 172.22.151.250:50582. Current token is Kind: NMToken, Service: 172.22.151.250:50582, Ident: (appAttemptId { application_id { id: 205 cluster_timestamp: 1449751495184 } attemptId: 1 } nodeId { host: "host-55" port: 50582 } appSubmitter: "hct" keyId: 1657307667)
15/12/15 16:55:27 DEBUG security.SaslRpcClient: Creating SASL DIGEST-MD5(TOKEN)  client to authenticate to service at default
15/12/15 16:55:27 DEBUG security.SaslRpcClient: Use TOKEN authentication for protocol ContainerManagementProtocolPB
15/12/15 16:55:27 DEBUG security.SaslRpcClient: SASL client callback: setting username: Cg4KCgjNARCQhLHfmCoQARIhChtzcmdzaS01NS5jb3JwLm1pY3Jvc29mdC5jb20QlosDGgNoY3Qgk4SilgY=
15/12/15 16:55:27 DEBUG security.SaslRpcClient: SASL client callback: setting userPassword
15/12/15 16:55:27 DEBUG security.SaslRpcClient: SASL client callback: setting realm: default
15/12/15 16:55:27 DEBUG security.SaslRpcClient: Sending sasl message state: INITIATE
token: "charset=utf-8,username=\"Cg4KCgjNARCQhLHfmCoQARIhChtzcmdzaS01NS5jb3JwLm1pY3Jvc29mdC5jb20QlosDGgNoY3Qgk4SilgY=\",realm=\"default\",nonce=\"SdoqyMQak5MKLXFZUGBc0NNr/+mIwiu9KkMJnO7Z\",nc=00000001,cnonce=\"YI7V6ndGGRj67G0dNvp1YPRGbTYo2pozDWoDw9vQ\",digest-uri=\"/default\",maxbuf=65536,response=68c4a28817f136bb9d1ec573d70c06ea,qop=auth"
auths {
  method: "TOKEN"
  mechanism: "DIGEST-MD5"
  protocol: ""
  serverId: "default"
}

15/12/15 16:55:27 DEBUG security.SaslRpcClient: Received SASL message state: SUCCESS
token: "rspauth=0697d1551d31f2d0f0b2d634263ecf73"

15/12/15 16:55:27 DEBUG ipc.Client: Negotiated QOP is :auth
15/12/15 16:55:27 DEBUG ipc.Client: IPC Client (1192487936) connection to host-55/172.22.151.250:50582 from appattempt_1449751495184_0205_000001 sending #35
15/12/15 16:55:27 DEBUG ipc.Client: IPC Client (1192487936) connection to host-55/172.22.151.250:50582 from appattempt_1449751495184_0205_000001: starting, having connections 2
15/12/15 16:55:27 DEBUG ipc.Client: IPC Client (1192487936) connection to host-55/172.22.151.250:50582 from appattempt_1449751495184_0205_000001 got value #35
15/12/15 16:55:27 DEBUG ipc.ProtobufRpcEngine: Call: startContainers took 15ms
15/12/15 16:55:27 DEBUG ipc.Client: IPC Client (1192487936) connection to host-55/172.22.151.250:50582 from appattempt_1449751495184_0205_000001: closed
15/12/15 16:55:27 DEBUG ipc.Client: IPC Client (1192487936) connection to host-55/172.22.151.250:50582 from appattempt_1449751495184_0205_000001: stopped, remaining connections 1
15/12/15 16:55:27 TRACE ipc.ProtobufRpcEngine: 55: Response <- host-55/172.22.151.250:50582: startContainers {succeeded_requests { app_attempt_id { application_id { id: 205 cluster_timestamp: 1449751495184 } attemptId: 1 } id: 62 }}
15/12/15 16:55:30 DEBUG yarn.ApplicationMaster: Sending progress
15/12/15 16:55:30 TRACE ipc.ProtobufRpcEngine: 27: Call -> host-40/172.22.151.235:8030: allocate {ask { priority { priority: 1 } resource_name: "*" capability { memory: 6758 virtual_cores: 8 } num_containers: 0 relax_locality: true } blacklist_request { } response_id: 21 progress: 0.1}
15/12/15 16:55:30 DEBUG ipc.Client: IPC Client (1192487936) connection to host-40/172.22.151.235:8030 from hct sending #36
15/12/15 16:55:30 DEBUG ipc.Client: IPC Client (1192487936) connection to host-40/172.22.151.235:8030 from hct got value #36
15/12/15 16:55:30 DEBUG ipc.ProtobufRpcEngine: Call: allocate took 2ms
15/12/15 16:55:30 TRACE ipc.ProtobufRpcEngine: 27: Response <- host-40/172.22.151.235:8030: allocate {response_id: 22 limit { memory: 7168 virtual_cores: 1 } num_cluster_nodes: 20}
15/12/15 16:55:30 DEBUG yarn.ApplicationMaster: Number of pending allocations is 0. Sleeping for 3000.
15/12/15 16:55:33 DEBUG yarn.ApplicationMaster: Sending progress
15/12/15 16:55:33 TRACE ipc.ProtobufRpcEngine: 27: Call -> host-40/172.22.151.235:8030: allocate {blacklist_request { } response_id: 22 progress: 0.1}
15/12/15 16:55:33 DEBUG ipc.Client: IPC Client (1192487936) connection to host-40/172.22.151.235:8030 from hct sending #37
15/12/15 16:55:33 DEBUG ipc.Client: IPC Client (1192487936) connection to host-40/172.22.151.235:8030 from hct got value #37
15/12/15 16:55:33 DEBUG ipc.ProtobufRpcEngine: Call: allocate took 2ms
15/12/15 16:55:33 TRACE ipc.ProtobufRpcEngine: 27: Response <- host-40/172.22.151.235:8030: allocate {response_id: 23 limit { memory: 7168 virtual_cores: 1 } num_cluster_nodes: 20}
15/12/15 16:55:33 DEBUG yarn.ApplicationMaster: Number of pending allocations is 0. Sleeping for 3000.
